---
title: "DATA 607 - Week 10 Assignment"
author: "Beshkia Kvarnstrom"
date: "2023-04-01"
output: rmdformats::readthedown
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 10 - Description
In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:

* Work with a different corpus of your choosing, and
* Incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).

As usual, please submit links to both an .Rmd file posted in your GitHub repository and to your code on rpubs.com.  You make work on a small team on this assignment.

## Primary example code from chapter 2 of "Text Mining with R"
```{r }
library(tidyverse)
library(tidytext)
library(textdata)
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidyr)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(lexicon)
library(readr)
```

#### The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one.
```{r, warnings=FALSE, warnings=FALSE}
get_sentiments("afinn")
```

```{r, message=FALSE, warnings=FALSE}
get_sentiments("bing")
```

```{r, message=FALSE, warnings=FALSE}
get_sentiments("nrc")
```

```{r, message=FALSE, warnings=FALSE}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(), chapter = cumsum(str_detect(text,  regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")


tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

```{r, message=FALSE, warnings=FALSE}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r, message=FALSE, warnings=FALSE}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
```{r, message=FALSE, warnings=FALSE}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice

```
```{r, message=FALSE, warnings=FALSE}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```
```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
```{r, message=FALSE, warnings=FALSE}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r, message=FALSE, warnings=FALSE}
get_sentiments("bing") %>% 
  count(sentiment)

```

```{r, message=FALSE, warnings=FALSE}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

```

```{r, message=FALSE, warnings=FALSE}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
```{r, message=FALSE, warnings=FALSE}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words

```

```{r, message=FALSE, warnings=FALSE}

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```
```{r, message=FALSE, warnings=FALSE}
tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

```{r, message=FALSE, warnings=FALSE}
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
```

```{r, message=FALSE, warnings=FALSE}
p_and_p_sentences$sentence[2]
#> [1] "by jane austen"
```

```{r, message=FALSE, warnings=FALSE}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())

```
```{r, message=FALSE, warnings=FALSE}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()

```
## My Corpus
#### I will be using the Loughran, SentiWordNet2 and SlangSD4 lexicon for my sentiment analysis of the tv show"The Office"
```{r, message=FALSE, warnings=FALSE}
library(schrute)
library(tibble)
library(childesr)
library(readr)

```

```{r}
# Lexicons SenticNet and SentiWordNet
office_sentiword <- hash_sentiment_sentiword
names(office_sentiword) <- c("word","score")

# Lexicons SlangSD
office_slangsd <- hash_sentiment_slangsd
names(office_slangsd) <- c("word","score")

```

```{r, message=FALSE, warnings=FALSE}
# Load dataset
theOfficeData <- schrute::theoffice
glimpse(theOfficeData)
```

```{r, message=FALSE, warnings=FALSE}
office_season <- theOfficeData %>%
filter(season %in%  c(5, 6))
#filter(season %in%  c(5, 6) &  episode == 1)

#Examine the dataset
glimpse(office_season)

```
```{r, message=FALSE, warnings=FALSE}
# Creating a subset of the data that will later be used to perform tidying
office_season <- office_season[c("text", "character", "index", "season", "episode", "episode_name", "imdb_rating", "total_votes", "air_date" )]

# Remove rows with missing values if there are any
office_season <- office_season %>%
  filter(!is.na(character), !is.na(text))

# Remove punctuation and convert to lowercase
office_season$text <- tolower(gsub("[^[:alnum:] ]", "", office_season$text))

office_season

```
#### Tidying the data
```{r, message=FALSE, warnings=FALSE}
tidy_office <- office_season %>%
  group_by(character) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```

```{r, message=FALSE, warnings=FALSE}
char_sentimentM <- tidy_office %>% 
  filter(character == "Michael")

char_sentimentM
```
```{r}
#sentiment <- get_sentiments("loughran")
#sentiment

```

```{r, message=FALSE, warnings=FALSE}
loughran_word_cnt <- char_sentimentM %>%
  inner_join(get_sentiments("loughran")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

loughran_word_cnt

```
```{r, message=FALSE, warnings=FALSE}
loughran_word_cnt %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free_y") +
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size=8))  +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

### Create sentiments based on each of the lexicons
```{r, message=FALSE, warnings=FALSE}
sentiment_sentiword <- char_sentimentM %>% 
  inner_join(office_sentiword, by = "word") %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>%
  mutate(method = "SENTIWORD")%>% 
 mutate(PosNeg = ifelse(sentiment > 0, "Positive", "Negative"))

sentiment_slangsd <- char_sentimentM %>% 
  inner_join(office_slangsd, by = "word") %>%
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(score)) %>%
  mutate(method = "SLANGSD")%>% 
 mutate(PosNeg = ifelse(sentiment > 0, "Positive", "Negative"))

sentiment_afinn <- char_sentimentM %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")%>% 
 mutate(PosNeg = ifelse(sentiment > 0, "Positive", "Negative"))

sentiment_loughran <- char_sentimentM %>% 
  inner_join(get_sentiments("loughran")) %>%  
  filter(sentiment %in% c("positive", "negative")) %>% 
  group_by(index = linenumber %/% 80) %>%
  mutate(method = "LOUGHRAN")%>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative) %>%
  mutate(PosNeg = ifelse(sentiment > 0, "Positive", "Negative"))

```

### Plot and compare each lexicons
```{r, message=FALSE, warnings=FALSE}
bind_rows(sentiment_sentiword, sentiment_slangsd, sentiment_afinn, sentiment_loughran) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

### plot sentiment for each lexicon (Positive & Negative)
```{r, message=FALSE, warnings=FALSE}
ggplot(sentiment_slangsd, aes(index, sentiment, fill = PosNeg)) +
  geom_col(show.legend = FALSE) + labs(title = "slangSD")
```

```{r, message=FALSE, warnings=FALSE}

ggplot(sentiment_sentiword, aes(index, sentiment, fill = PosNeg)) +
  geom_col(show.legend = FALSE) + labs(title = "SENTIWORD")
```

```{r, message=FALSE, warnings=FALSE}

ggplot(sentiment_loughran, aes(index, sentiment, fill = PosNeg)) +
  geom_col(show.legend = FALSE) + labs(title = "LOUGHRAN")
```

```{r, message=FALSE, warnings=FALSE}

ggplot(sentiment_afinn, aes(index, sentiment, fill = PosNeg)) +
  geom_col(show.legend = FALSE) + labs(title = "AFINN")
```

### Display the most freuently used words joining on loughran lexicon
```{r, message=FALSE, warnings=FALSE}
loughran_lex_count <- tidy_office %>%
  inner_join(get_sentiments("loughran")) %>%
  filter(!is.na(sentiment)) %>%
  count(word, sentiment, sort = TRUE)
    
loughran_lex_count
```
```{r, message=FALSE, warnings=FALSE}
loughran_lex_count %>%
  group_by(sentiment) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free_y") +
  labs(y = "Total Sentiment",
       x = "Words From The Office Script") +
  theme(axis.text.x = element_text(angle = 70, hjust = 1, size=8))  +
  coord_flip()

```

```{r, message=FALSE, warnings=FALSE}
office_word_freq <- tidy_office %>%
  inner_join(get_sentiments("loughran")) %>%
  filter(!is.na(sentiment)) %>%
  filter(sentiment %in% c("positive", "negative"))%>%

  count(word, sentiment, sort = TRUE)
    
office_word_freq

```
### Comparison Word Cloud of loughran sentiment values positive, "negative, superfluous and litigious
```{r, message=FALSE, warnings=FALSE}
tidy_office %>%
  inner_join(get_sentiments("loughran")) %>%
  filter(sentiment %in% c("positive", "negative", "superfluous", "litigious"))%>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("purple", "green"),
                   max.words = 100)
```

### Comparison Word Cloud of loughran sentiment values positive, "negative, superfluous and litigious joing on stop words 
```{r, message=FALSE, warnings=FALSE}
tidy_office %>%
  inner_join(get_sentiments("loughran")) %>%
  filter(sentiment %in% c("positive", "negative", "superfluous", "litigious"))%>%
    anti_join(stop_words) %>% 
    count(word) %>%
    with(wordcloud(word, colors = c("purple", "green"),n, max.words = 100))
```

#### Plot the most frequently used words in the Episode that I am analyzing
```{r}
top_10 <- office_word_freq %>%
  filter(n >= 20)

ggplot(top_10, aes(x = word, y = n, fill = sentiment)) + 
theme(axis.text.x = element_text(angle = 70, hjust = 1, size=8))  +
  labs(title = 'MOST FREQUENTLY USED WORDS')+
  labs(x = "Frequently used words",
       y = "Total Sentiment") +
geom_col()

```

## Conclusion
After reviewing the data for the tv show "The Office" and performing sentiment analysis, I can conclude that there were more positive and uncertain words. Overall SlangSD showed more negative word usage.

## Reference
https://medium.com/nerd-for-tech/sentiment-analysis-lexicon-models-vs-machine-learning-b6e3af8fe746#:~:text=AFINN%20Lexicon%20is%20the%20most,along%20with%20it's%20polarity%20score.
